{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Векторная модель [Vector Space Model]\n",
    "\n",
    "Все (слово, документ) – вектор. Документ $doc$ может быть представлен вектором в пространстве слов:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\vec{doc} = (f_1, \\ldots, f_{|V|}), \n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "V - vocabulary, word \\in V\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|   | $d_1$ | $d_2$   | $\\ldots$  | $d_C$ |\n",
    "|---|-------|---|---|----|\n",
    "| $w_1$  |  $f_{11}$     |   $\\ldots$ |   |    |\n",
    "| $w_2$  |     $\\ldots$   |   |   |    |\n",
    "| $\\ldots$  |       |   |   |    |\n",
    "| $w_V$  |       |   |   | $f_{VC}$   |\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "C - corpus, doc \\in C\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "f(word, doc) - \\text{вес слова}~word~\\text{в}~doc\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Веса \n",
    "\n",
    "* $f(word, doc) =\n",
    "  \\begin{cases}\n",
    "    1 & \\text{if}~word~\\in~doc\\\\\n",
    "    0, otherwise \\\\\n",
    "  \\end{cases}$\n",
    "\n",
    "\n",
    "* $f(word, doc) = tf(word, doc) = \\frac{\\text{count}(word,doc)}{\\sum_i \\text{count}(word_i,doc) } $\n",
    "\n",
    "\n",
    "* $f(word, doc) = \\text{tf}(word, doc) \\times \\text{idf}(word) = $\n",
    "\n",
    "= $\\frac{\\text{count}(word,doc)}{\\sum_i \\text{count}(word_i,doc) }  \\times \\log \\frac{|C|}{\\text{count}(doc | word \\in doc)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\vec{doc}$ может быть использован:\n",
    "* как входное представление документа для любого алгоритма машинного обучения \n",
    "* для определения близости между документами\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{similarity}(d_i, d_j) = \\text{cosine}(\\vec{doc}_i, \\vec{doc}_j)  = \\frac{\\sum_k f_{ki} f_{kj}}{\\sqrt{\\sum_k f_{ki}^2 } \\sqrt{\\sum_k f_{kj}^2 } }\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/cos.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Слово может быть тоже представлено вектором, но в пространстве документов $|C| << |V|$.\n",
    "\n",
    "|   | $d_1$ | $d_2$   | $\\ldots$  | $d_C$ |\n",
    "|---|-------|---|---|----|\n",
    "| $w_1$  |  $f_{11}$     |   $\\ldots$ |   |    |\n",
    "| $w_2$  |     $\\ldots$   |   |   |    |\n",
    "| $\\ldots$  |       |   |   |    |\n",
    "| $w_V$  |       |   |   | $f_{VC}$   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому чаще в качестве координат в векторном пространстве рассматривают соседние слова, т.н. контексты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left and right contexts to the word \"gravity\"\n",
    "from nltk.text import Text  \n",
    "import re\n",
    "\n",
    "regex = r'\\w+'\n",
    "\n",
    "gravity =  ' '.join(open('data/Gravity.txt').readlines())\n",
    "tokens = re.findall(regex, gravity.lower())\n",
    "txt = Text(tokens)\n",
    "txt.concordance('gravity', lines = 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|   | $c_1$ | $c_2$   | $\\ldots$  | $c_V$ |\n",
    "|---|-------|---|---|----|\n",
    "| $w_1$  |  $f_{11}$     |   $\\ldots$ |   |    |\n",
    "| $w_2$  |     $\\ldots$   |   |   |    |\n",
    "| $\\ldots$  |       |   |   |    |\n",
    "| $w_V$  |       |   |   | $f_V$   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Веса\n",
    "\n",
    "* $f(word, context) =\n",
    "  \\begin{cases}\n",
    "    1 & \\text{если слово встречается в контексте}\\\\\n",
    "    0, otherwise \\\\\n",
    "  \\end{cases}$\n",
    "\n",
    "\n",
    "* $f(word, context) = freq(word, context) = \\frac{\\text{count}(word,context)}{|D|}$, $D$ - все пары ($word, context$)\n",
    "\n",
    "\n",
    "* $f(word, context) = PPMI(word, context)\n",
    "  \\begin{cases}\n",
    "    PMI(word, context) = \\frac{\\text{count}(word,context) |D|}{\\text{count}(word), \\text{count}(count)} \\\\\n",
    "    0, otherwise \\\\\n",
    "  \\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почему PPMI?\n",
    "* $freq$(the, cat) > $freq$(cute, cat)\n",
    "* $PPMI$(the, cat) < $PPMI$(cute, cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проблемы\n",
    "1. матрицы слишком разряжены;\n",
    "2. невозможно установить семантическую близость между словами: слова ``кошка'' и ``собака'' по смыслу похожи (до какой-то степени), но встречаются в разных контекстах, то есть:\n",
    "\n",
    "$cosine$(кошка, собака) $\\rightarrow 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Снижение размерности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждое слово представляется вектором размерности $d$:\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\\vec{word}  \\in \\mathbb{R}^{|V|} \\rightarrow \\vec{e}_{word}\\in \\mathbb{R}^d\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сингулярное разложение\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "M = U S V^{T},\n",
    "\\end{equation*}\n",
    "\n",
    "матрица $M$ – матрица слово-контекст, $U$ – матрица левых сингулярных векторов, $S$  – матрица сингулярных чисел, $V^T$ – матрица правых сингулярных векторов. \n",
    "\n",
    "После обнуления всех, кроме первых $d$ сингулярных чисел, положим $M_d = U_d S_d V^{T}_d$ – лучшая аппроксимация матрицы $M$ ранга $d$ в смысле МНК. \n",
    "\n",
    "$M_d = U_d \\times S_d \\times v^{T}_d$\n",
    "\n",
    "\n",
    "\n",
    "Эмбеддинги: $U_d \\times \\sqrt{S_d}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/svd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зачем нужны эмбеддинги:\n",
    "* решать лингвистические задачи\n",
    "* подавать на вход нейронным сетям"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/word2vec-gender-relation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как оценить качество эмбеддингов:\n",
    "\n",
    "* **синонимы**: найти ближайшее по смыслу слово\n",
    "* **ассоциации**: man : king = woman : queen \n",
    "* **лишнее слово**: cat dog fish cow pizza $\\rightarrow$ pizza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec \n",
    "\n",
    "\n",
    "Две архитектуры:\n",
    "* Continious bag of words\n",
    "* Skip-gram\n",
    "\n",
    "Две оптимизационные задачи:\n",
    "* Negative sampling \n",
    "* Hierarchical softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предсказание наблюдаемой пары (word, context)  $\\in D$:\n",
    "\n",
    "$P(D=1|word,context) = \\frac{1}{1+e^{-s(word,context)}} \\rightarrow  \\max $\n",
    "\n",
    "Предсказание ненаблюдаемой пары (word, context)  $\\in \\bar{D}$:\n",
    "\n",
    "$P(D=0|word,context) = \\frac{1}{1+e^{-s(word,context)}} \\rightarrow \\max $\n",
    "\n",
    "Выбираем $k$ неналюдаемых пар из $\\bar{D}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все вместе $L(D,\\bar{D}) = P(D=1|word,context) + P(D=0|word,context)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Continious bag of words**: предсказываем слово в контексте $c_{-2}, c_{-1},  c_{1}, c_{2}$, контексты зависимы\n",
    "\n",
    "<img src=\"img/cbow.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x$ – one-hot вектора размерности  $|V|$\n",
    "\n",
    "скрытый слой: $W \\in \\mathbb{R} ^ {V \\times N}$ – матрица весов\n",
    "\n",
    "$h = x^T W = v_{w}$ - векторное представление входного слова\n",
    "\n",
    "выходной слой: $W' \\in \\mathbb{R} ^ {N \\times V}$ – матрица весов. \n",
    "\n",
    "$ u_j = v_{w}^{'T} h $ - score for each word in the vocabulary\n",
    "\n",
    "$ p(w|c) = \\frac{u_c}{\\sum_j u_j}$ - функция softmax используется для итоговой классификации на итоговом слое\n",
    "\n",
    "функция потерь: $\\max(w|c) = u_{c^*} - log \\sum_{j \\in V} exp(u_j)$\n",
    "\n",
    "Эмбеддинг слова $word_i$ is $\\vec{word}_i = x \\times W$, строка в матрицы весов размерности $N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Skip-gram**: предсказываем контекст по слову, контексты независимы\n",
    "\n",
    "\n",
    "функция потерь: $ - \\log \\prod \\frac{ exp(u_{c,j*}) }{ \\sum_{j'} exp(u_{j'})} = -\\sum_j u_{c,j*} + C \\log \\sum_{j'} exp(u_{j'})  $\n",
    "\n",
    "\n",
    "<img src=\"img/sgns.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim для Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import *\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter, defaultdict\n",
    "random.seed(1228)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем лемматизированные статьи без стоп-слов и создаем массив текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "import re\n",
    "import regex\n",
    "\n",
    "\n",
    "m = Mystem()\n",
    "\n",
    "\n",
    "\n",
    "def words_only(text):\n",
    "    try:\n",
    "        tmp = re.sub(r'\\W+', ' ', text)\n",
    "        tmp = regex.sub(u'[\\p{Latin}]', u'', tmp)\n",
    "        return ' '.join(tmp.split())\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "\n",
    "def lemmatize(text, mystem=m):\n",
    "    try:\n",
    "        return \"\".join(m.lemmatize(text)).strip()  \n",
    "    except:\n",
    "        return \" \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neg = pd.read_csv(\"data/negative.csv\", sep=';', header = None, usecols = [3])\n",
    "df_pos = pd.read_csv(\"data/positive.csv\", sep=';', header = None, usecols = [3])\n",
    "df_neg['sent'] = 'neg'\n",
    "df_pos['sent'] = 'pos'\n",
    "df = pd.concat([df_neg, df_pos])\n",
    "df.columns = ['text', 'sent']\n",
    "df.text = df.text.apply(words_only)\n",
    "df.text = df.text.apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos[3].tolist()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [df.text.iloc[i].split() for i in range(len(df))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение модели в gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(texts, size=300, window=5, min_count=5, workers=4)\n",
    "model.save(\"sent_w2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model['корпоратив']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"корпоратив\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=[\"хорошо\",\"плохой\"], negative=[\"хороший\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=[\"тепло\",\"зима\"], negative=[\"лето\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.doesnt_match(\"борщ сметана макароны пирожок консомэ кошка\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация пространства слов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_words = []\n",
    "from collections import Counter\n",
    "fd = Counter()\n",
    "for text in texts:\n",
    "    fd.update(text)\n",
    "for i in fd.most_common(1000):\n",
    "    top_words.append(i[0])\n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_vec = model[top_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "top_words_tsne = tsne.fit_transform(top_words_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "p = figure(tools=\"pan,wheel_zoom,reset,save\",\n",
    "           toolbar_location=\"above\",\n",
    "           title=\"word2vec T-SNE for most common words\")\n",
    "\n",
    "source = ColumnDataSource(data=dict(x1=top_words_tsne[:,0],\n",
    "                                    x2=top_words_tsne[:,1],\n",
    "                                    names=top_words))\n",
    "\n",
    "p.scatter(x=\"x1\", y=\"x2\", size=8, source=source)\n",
    "\n",
    "labels = LabelSet(x=\"x1\", y=\"x2\", text=\"names\", y_offset=6,\n",
    "                  text_font_size=\"8pt\", text_color=\"#555555\",\n",
    "                  source=source, text_align='center')\n",
    "p.add_layout(labels)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кластеризация слов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(top_words_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import  ward, dendrogram\n",
    "\n",
    "linkage_matrix = ward(dist) \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 100)) \n",
    "ax = dendrogram(linkage_matrix, orientation=\"right\", labels=top_words);\n",
    "\n",
    "plt.tick_params(\\\n",
    "    axis= 'x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom='off',      # ticks along the bottom edge are off\n",
    "    top='off',         # ticks along the top edge are off\n",
    "    labelbottom='off')\n",
    "\n",
    "plt.tight_layout() \n",
    "\n",
    "plt.savefig('w2v_clusters.png', dpi=200) #save figure as ward_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация текстов \n",
    "\n",
    "По мотивам http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  word2vec, doc2vec и fasttext \n",
    "\n",
    "\n",
    "1. word2vec – векторное представление слова\n",
    "2. как сделать вектор документа?\n",
    "    * усреднить все вектора слов\n",
    "    * усреднить все вектора слов с $tf-idf$ весами\n",
    "    * doc2vec\n",
    "3. fasttext – векторное представление $n$-грам\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.text.tolist()\n",
    "y = df.sent.tolist()\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33)\n",
    "print (\"total train examples %s\" % len(y_train))\n",
    "print (\"total test examples %s\" % len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(w2v.popitem()[1])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(w2v.popitem()[1])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc_w2v = Pipeline([\n",
    "    (\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)),\n",
    "    (\"extra trees\", RandomForestClassifier(n_estimators=20))])\n",
    "rfc_w2v_tfidf = Pipeline([\n",
    "    (\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v)),\n",
    "    (\"extra trees\", RandomForestClassifier(n_estimators=20))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_w2v.fit(X_train,y_train)\n",
    "pred = rfc_w2v.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision: {0:6.2f}\".format(precision_score(y_test, pred, average='macro')))\n",
    "print(\"Recall: {0:6.2f}\".format(recall_score(y_test, pred, average='macro')))\n",
    "print(\"F1-measure: {0:6.2f}\".format(f1_score(y_test, pred, average='macro')))\n",
    "print(\"Accuracy: {0:6.2f}\".format(accuracy_score(y_test, pred)))\n",
    "print(classification_report(y_test, pred))\n",
    "labels = rfc_w2v.classes_\n",
    "\n",
    "\n",
    "sns.heatmap(data=confusion_matrix(y_test, pred), annot=True, fmt=\"d\", cbar=False, xticklabels=labels, yticklabels=labels)\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_w2v_tfidf.fit(X_train,y_train)\n",
    "pred = rfc_w2v_tfidf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision: {0:6.2f}\".format(precision_score(y_test, pred, average='macro')))\n",
    "print(\"Recall: {0:6.2f}\".format(recall_score(y_test, pred, average='macro')))\n",
    "print(\"F1-measure: {0:6.2f}\".format(f1_score(y_test, pred, average='macro')))\n",
    "print(\"Accuracy: {0:6.2f}\".format(accuracy_score(y_test, pred)))\n",
    "print(classification_report(y_test, pred))\n",
    "labels = rfc_w2v.classes_\n",
    "\n",
    "\n",
    "sns.heatmap(data=confusion_matrix(y_test, pred), annot=True, fmt=\"d\", cbar=False, xticklabels=labels, yticklabels=labels)\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## paragraph2vec aka doc2vec\n",
    "\n",
    "\n",
    "word2vec с дополнительной меткой id документа\n",
    "\n",
    "![img](img/w2v_4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_texts = [text.split() for text in X]\n",
    "idx = [str(i) for i in range(len(X))]\n",
    "\n",
    "docs = []\n",
    "for i in range(len(X)):\n",
    "    docs.append(TaggedDocument(splitted_texts[i], [idx[i]]))\n",
    "\n",
    "\n",
    "model = Doc2Vec(size=300, window=5, min_count=5, workers=8, alpha=0.025, min_alpha=0.01, dm=0)\n",
    "model.build_vocab(docs)\n",
    "model.train(docs, total_examples=len(docs), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2VecVectorizer(object):\n",
    "    def __init__(self, d2v_model):\n",
    "        self.d2v_model = d2v_model\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([self.d2v_model.infer_vector(text.split()) for text in X])\n",
    "\n",
    "\n",
    "rfc_d2v = Pipeline([\n",
    "    (\"word2vec vectorizer\", Doc2VecVectorizer(model)),\n",
    "    (\"extra trees\", RandomForestClassifier(n_estimators=20))])\n",
    "\n",
    "rfc_d2v.fit(X_train,y_train)\n",
    "pred = rfc_d2v.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision: {0:6.2f}\".format(precision_score(y_test, pred, average='macro')))\n",
    "print(\"Recall: {0:6.2f}\".format(recall_score(y_test, pred, average='macro')))\n",
    "print(\"F1-measure: {0:6.2f}\".format(f1_score(y_test, pred, average='macro')))\n",
    "print(\"Accuracy: {0:6.2f}\".format(accuracy_score(y_test, pred)))\n",
    "print(classification_report(y_test, pred))\n",
    "labels = rfc_w2v.classes_\n",
    "\n",
    "\n",
    "sns.heatmap(data=confusion_matrix(y_test, pred), annot=True, fmt=\"d\", cbar=False, xticklabels=labels, yticklabels=labels)\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttext \n",
    "\n",
    "Слово $w$ представляем символьными $n$-грамами: \n",
    "\n",
    "$n=3$, $G_{where} = \\_wh, whe, her, re\\_, \\_where\\_$\n",
    "\n",
    "$sim_{w2v}(u,v) = <u,v>$\n",
    "\n",
    "\n",
    "$ sim_{ft} (u,v) = \\sum_{e \\in G_u} \\sum_{g \\in G_v} <e,v>$\n",
    "\n",
    "\n",
    "https://github.com/facebookresearch/fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "with open('data.train.txt', 'w+') as outfile:\n",
    "    for i in range(len(X_train)):\n",
    "        outfile.write('__label__' + y_train[i] + ' '+ X_train[i] + '\\n')\n",
    "    \n",
    "\n",
    "with open('test.txt', 'w+') as outfile:\n",
    "    for i in range(len(X_test)):\n",
    "        outfile.write('__label__' + y_test[i] + ' ' + X_test[i] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = fasttext.supervised('data.train.txt', 'model')\n",
    "result = classifier.test('test.txt')\n",
    "print('P@1:', result.precision)\n",
    "print('R@1:', result.recall)\n",
    "print('Number of examples:', result.nexamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = classifier.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Precision: {0:6.2f}\".format(precision_score(y_test, pred, average='macro')))\n",
    "print(\"Recall: {0:6.2f}\".format(recall_score(y_test, pred, average='macro')))\n",
    "print(\"F1-measure: {0:6.2f}\".format(f1_score(y_test, pred, average='macro')))\n",
    "print(\"Accuracy: {0:6.2f}\".format(accuracy_score(y_test, pred)))\n",
    "print(classification_report(y_test, [i[0] for i in pred]))\n",
    "labels = rfc_w2v.classes_\n",
    "\n",
    "\n",
    "sns.heatmap(data=confusion_matrix(y_test, pred), annot=True, fmt=\"d\", cbar=False, xticklabels=labels, yticklabels=labels)\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
