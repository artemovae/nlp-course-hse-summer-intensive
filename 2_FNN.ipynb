{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Классификация текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Сеть прямого распространения  для классификации текстов\n",
    "\n",
    "\n",
    "![title](img/mlp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* $x$ - входное векторное представление текста\n",
    "* $h$ – скрытые слои с нелинейными функциями активации\n",
    "* $y$ – выходы, как правило, один $y$ соответствует одной метке класса \n",
    "\n",
    "$NN_{MLP2}(x) = y$\n",
    "\n",
    "$h_1 = g^1(xW^1 + b^1)$\n",
    "\n",
    "$h_2 = g^2(h^1 W^2 + b^2)$\n",
    "\n",
    "$y = h^2 W^3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Нелинейные функции активации\n",
    "\n",
    "![title](img/activation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение сети \n",
    "### Алгоритм обратного распространения ошибки \n",
    "\n",
    "Ошибка: cross entropy: $\\text{loss}(y_{true}, \\hat{y}_{pred}) = \\sum y_{true} \\log(\\hat{y}_{pred})$ \n",
    "\n",
    "1. Прямой проход:\n",
    "    * вычислить $\\hat{y}_{pred}$ с текущими весами на скрытых слоях\n",
    "    * оценить $\\text{loss}(y_{true}, \\hat{y}_{pred})$\n",
    "\n",
    "2. Обратный проход:\n",
    "    * оценить  $\\Delta W_h$ на каждом скрытом слое\n",
    "    \n",
    "    $\\Delta W_h = \\frac {\\partial  \\text{loss}}{\\partial W_{H}} = \\frac{\\partial \\text{loss}}{\\partial \\hat{y}_{pred}}  \\frac{\\partial \\hat{y}_{pred}}{\\partial W_{H}} $\n",
    "    \n",
    "    * обновление весов: $ \\Delta W_H =-\\eta {\\frac {\\partial \\text{loss} } {\\partial W_H} }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### dropout-регуляризация\n",
    "\n",
    "$NN_{MLP2}(x) = y$\n",
    "\n",
    "$h_1 = g^1(xW^1 + b^1)$\n",
    "\n",
    "$m^1 $~$ Bernouli(r^1)$\n",
    "\n",
    "$\\hat{h^1} = m^1 \\odot h^1$\n",
    "\n",
    "$h_2 = g^2(\\hat{h^1} W^2 + b^2)$\n",
    "\n",
    "$m^2 $~$  Bernouli(r^2)$\n",
    "\n",
    "$\\hat{h^2} = m^2 \\odot h^2$\n",
    "\n",
    "$y =\\hat{h^2} W^3$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Векторное представление текста \n",
    "\n",
    "\n",
    "1. Мешок слов [Bag of Words, BoW]\n",
    "    * $|\\text{word} \\in V| = N$ – словарь\n",
    "    * $x \\in D$ – документ, $|x| = k$ \n",
    "    * $\\vec{x}$ – $N$-мерный вектор, $\\vec{x}_i = f(\\text{word}_i, x_i)$, в котором $k$  ненулевых компонент\n",
    " \n",
    "\n",
    "2. Распределенное представление слов [Continuous Bag of Words, CBoW])\n",
    "    * one-hot кодировка: каждое слово $\\text{word}$ – $N$-мерный вектор, $\\overrightarrow{\\text{word}}_i = 1$, иначе – 0\n",
    "    * плотные вектора – эмбеддинги: каждое слово $\\text{word}$ – $d$-мерный вектор, $\\overrightarrow{\\text{word}}_i \\in \\mathbb{R}$\n",
    "\t\n",
    "    Матрица эмбеддингов: $E \\in \\mathbb{R}^{|V| \\times d}$\n",
    "\t\n",
    "    * $\\text{CBOW}(x) = \\frac{1}{k} \\sum_i^k E_i $\n",
    "    * $\\text{x} = [\\overrightarrow{\\text{word}}_1  ,\\ldots, \\overrightarrow{\\text{word}}_k ]$\n",
    "\n",
    "\n",
    "#### Padding\n",
    "Входные тексты имеют переменную длинну, что неудобно, поэтому предположим, что они все состоят из одинакового количества слов, только часть из этих слов – баластные символы pad.\n",
    "\n",
    "\n",
    "#### Неизвестные слова (OOV)\n",
    "Если в тестовом множестве встретилось неизвестное слово, то можно \n",
    "* заменить его на pad;\n",
    "* заменить его на unk.  Однако в обучающем множестве unk никогда не встречается, поэтому его нужно добавить в обучающее множество искусственным образом. \n",
    "\n",
    "\n",
    "#### Word dropout - регуляризация \n",
    "Заменяем каждое слово на unk с вероятностью $\\frac{\\alpha}{|V| + \\alpha}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "\n",
    "from keras.layers import Embedding, Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "random.seed(1228)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "m = Mystem()\n",
    "\n",
    "\n",
    "regex = re.compile(\"[А-Яа-я:=!\\)\\()A-z\\_\\%/|]+\")\n",
    "\n",
    "def words_only(text, regex=regex):\n",
    "    try:\n",
    "        return \" \".join(regex.findall(text))\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "\n",
    "def lemmatize(text, mystem=m):\n",
    "    try:\n",
    "        return \"\".join(m.lemmatize(text)).strip()  \n",
    "    except:\n",
    "        return \" \"\n",
    "\n",
    "\n",
    "df_neg = pd.read_csv(\"data/negative.csv\", sep=';', header = None, usecols = [3])\n",
    "df_pos = pd.read_csv(\"data/positive.csv\", sep=';', header = None, usecols = [3])\n",
    "df_neg['sent'] = 'neg'\n",
    "df_pos['sent'] = 'pos'\n",
    "df = pd.concat([df_neg, df_pos])\n",
    "df.columns = ['text', 'sent']\n",
    "df.text = df.text.apply(words_only)\n",
    "df.text = df.text.apply(lemmatize)\n",
    "\n",
    "\n",
    "X = df.text.tolist()\n",
    "y = df.sent.tolist()\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "X_text_train, X_text_test, y_train, y_test = train_test_split(X,y, test_size=0.33)\n",
    "print (\"total train examples %s\" % len(y_train))\n",
    "print (\"total test examples %s\" % len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "TEXT_LENGTH = 10\n",
    "VOCABULARY_SIZE = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "DIMS = 250\n",
    "MAX_FEATURES = 5000\n",
    "\n",
    "batch_size = 32\n",
    "nb_filter = 250\n",
    "filter_length = 3\n",
    "hidden_dims = 250\n",
    "nb_epoch = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Сеть прямого распространения\n",
    "\n",
    "### BoW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(X_text_train)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(X_text_train)\n",
    "X_train = tokenizer.sequences_to_matrix(sequences, mode='count')\n",
    "sequences = tokenizer.texts_to_sequences(X_text_test)\n",
    "X_test = tokenizer.sequences_to_matrix(sequences, mode='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print('First seq:',sequences[0])\n",
    "print('First doc:',X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(['pos', 'neg'])\n",
    "y_train_cat = np_utils.to_categorical(le.transform(y_train), 2)\n",
    "y_test_cat = np_utils.to_categorical(le.transform(y_test), 2)\n",
    "\n",
    "print(y_train_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(MAX_FEATURES,), activation = 'relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train_cat, epochs=nb_epoch, batch_size=batch_size,  validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pred = model.predict_classes(X_test)\n",
    "pred = le.inverse_transform(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision: {0:6.2f}\".format(precision_score(y_test, pred, average='macro')))\n",
    "print(\"Recall: {0:6.2f}\".format(recall_score(y_test, pred, average='macro')))\n",
    "print(\"F1-measure: {0:6.2f}\".format(f1_score(y_test, pred, average='macro')))\n",
    "print(\"Accuracy: {0:6.2f}\".format(accuracy_score(y_test, pred)))\n",
    "print(classification_report(y_test, pred))\n",
    "\n",
    "\n",
    "\n",
    "sns.heatmap(data=confusion_matrix(y_test, pred), annot=True, fmt=\"d\", cbar=False, xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBoW – случайно инициализированные эмбеддинги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(X_text_train)\n",
    "X_train = pad_sequences(sequences, maxlen=TEXT_LENGTH)\n",
    "sequences = tokenizer.texts_to_sequences(X_text_test)\n",
    "X_test = pad_sequences(sequences, maxlen=TEXT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(VOCABULARY_SIZE, EMBEDDING_DIM, input_length=TEXT_LENGTH, trainable = True))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train_cat, epochs=nb_epoch, batch_size=batch_size,  validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict_classes(X_test)\n",
    "pred = le.inverse_transform(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision: {0:6.2f}\".format(precision_score(y_test, pred, average='macro')))\n",
    "print(\"Recall: {0:6.2f}\".format(recall_score(y_test, pred, average='macro')))\n",
    "print(\"F1-measure: {0:6.2f}\".format(f1_score(y_test, pred, average='macro')))\n",
    "print(\"Accuracy: {0:6.2f}\".format(accuracy_score(y_test, pred)))\n",
    "print(classification_report(y_test, pred))\n",
    "\n",
    "\n",
    "\n",
    "sns.heatmap(data=confusion_matrix(y_test, pred), annot=True, fmt=\"d\", cbar=False, xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBoW – предобученные эмбеддинги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "emb_path = '/NLP/embeddings/wiki.ru.vec'\n",
    "\n",
    "words = []\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(emb_path)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    if len(values) == 301:\n",
    "        word = values[0]\n",
    "        words.append(word)\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=TEXT_LENGTH,\n",
    "                            trainable=False))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train_cat, epochs=nb_epoch, batch_size=batch_size,  validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision: {0:6.2f}\".format(precision_score(y_test, pred, average='macro')))\n",
    "print(\"Recall: {0:6.2f}\".format(recall_score(y_test, pred, average='macro')))\n",
    "print(\"F1-measure: {0:6.2f}\".format(f1_score(y_test, pred, average='macro')))\n",
    "print(\"Accuracy: {0:6.2f}\".format(accuracy_score(y_test, pred)))\n",
    "print(classification_report(y_test, pred))\n",
    "\n",
    "\n",
    "\n",
    "sns.heatmap(data=confusion_matrix(y_test, pred), annot=True, fmt=\"d\", cbar=False, xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Convolution1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.layers import  concatenate\n",
    "\n",
    "\n",
    "nb_filter = 50\n",
    "filter_lengths = [1,2,3]\n",
    "hidden_dims = 100\n",
    "nb_epoch = 20\n",
    "\n",
    "\n",
    "\n",
    "words_input = Input(shape=(TEXT_LENGTH,), dtype='int32', name='words_input')\n",
    "\n",
    "\n",
    "\n",
    "#Our word embedding layer\n",
    "wordsEmbeddingLayer = Embedding(embedding_matrix.shape[0],\n",
    "                    embedding_matrix.shape[1],                                     \n",
    "                    weights=[embedding_matrix],\n",
    "                    trainable=False)\n",
    "\n",
    "\n",
    "words = wordsEmbeddingLayer(words_input)\n",
    "\n",
    "#Now we add a variable number of convolutions\n",
    "words_convolutions = []\n",
    "for filter_length in filter_lengths:\n",
    "    words_conv = Convolution1D(filters=nb_filter,\n",
    "                            kernel_size=filter_length,\n",
    "                            padding='same',\n",
    "                            activation='relu',\n",
    "                            strides=1)(words)\n",
    "                            \n",
    "    words_conv = GlobalMaxPooling1D()(words_conv)      \n",
    "    \n",
    "    words_convolutions.append(words_conv)  \n",
    "\n",
    "output = concatenate(words_convolutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We add a vanilla hidden layer together with dropout layers:\n",
    "output = Dropout(0.5)(output)\n",
    "output = Dense(hidden_dims, activation='tanh', kernel_regularizer=keras.regularizers.l2(0.01))(output)\n",
    "output = Dropout(0.25)(output)\n",
    "\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "output = Dense(2, activation='sigmoid',  kernel_regularizer=keras.regularizers.l2(0.01))(output)\n",
    "\n",
    "model = Model(inputs=[words_input], outputs=[output])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train_cat, epochs=nb_epoch, batch_size=batch_size,  validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
